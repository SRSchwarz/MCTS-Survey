\section{Conclusion}
\label{sec:conclusion}
This paper gives an overview of research developments concerning the tree search algorithm Monte Carlo Tree Search. The algorithm iteratively executes four steps: In each iteration the current tree (usually a subset of the \textit{game tree}) is traversed starting from the root until a (new or existing) node is selected and expanded. The exact details of this process are defined by the tree policy. Now the default policy is used to run a simulation whose results are then backpropagated up the tree. This way values are assigned to the states contained in the tree. When a computational budget is exhausted the action leading to the most valuable state from the root of the tree is returned as the result. The most important implementation of this basic algorithm is UCT which models the node selection in the first step as a Multi-armed Bandit problem and aims to maximize the expected value of the selected node. There are multiple ways of achieving this. Section \ref{ss:mab} gives algorithms that solve this problem in various settings (such as when only a limited number of turns is available or a dynamic change of rewards is possible). The final step of backpropagation and deciding which recorded statistics are updated is also open to modification. Section \ref{ss:heuristics} (among more general aspects) presents a heuristic called AMAF which does exactly this and extends the nodes that are modified after each simulation.

Other variations of MCTS describe its application in different settings. Section \ref{ss:multi_agents} examines two modifications that allow for it to be executed on multiple agents that (unlike in the game-theoretic focus of most research) cooperate and communicate to achieve a common goal mostly related to planning. Section \ref{ss:real_time} is about the real-time setting where time is limited and special heuristics allow for the reuse of results from previous iterations or to aim to avoid running into dead-ends. Section \ref{ss:evo_alg} describes ways of combining MCTS with the paradigm of evolutionary algorithms which take inspiration from biology to evaluate the fitness (performance) of multiple, slightly different MCTS-instances and aim to increase this fitness both by randomly modifying and deterministically combining the best available instances. 

MCTS was made famous by its application in the game of Go. Its most recent breakthrough as part of AlphaGo \cite{silver2017mastering} (a neural network that was the first AI to beat a human champion) is described in Section \ref{ss:go}. Further use cases such as in the design of neural network architectures, as database optimization or for (more theoretical) graph matching algorithms are discussed in Section \ref{ss:nn} and Section \ref{ss:others} respectively. Such use cases (as well as others) have shown MCTS to be effective in large domains or when no additional domain knowledge is available. While MCTS does not always perform better than comparable search algorithms, variations like UCT generally address the exploration-exploitation tradeoff very effectively. At the same time they are open to a large number of modifications to fine-tune the performance if additional domain knowledge exists or specific properties are required. This results in a large body of current and possible future research as MCTS gets applied to an ever-increasing range of domains.
